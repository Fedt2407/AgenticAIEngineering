{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And now - Week 3 Day 3\n",
    "\n",
    "## AutoGen Core\n",
    "\n",
    "Something a little different.\n",
    "\n",
    "This is agnostic to the underlying Agent framework\n",
    "\n",
    "You can use AutoGen AgentChat, or you can use something else; it's an Agent interaction framework.\n",
    "\n",
    "From that point of view, it's positioned similarly to LangGraph.\n",
    "\n",
    "### The fundamental principle\n",
    "\n",
    "Autogen Core decouples an agent's logic from how messages are delivered.  \n",
    "The framework provides a communication infrastructure, along with agent lifecycle, and the agents are responsible for their own work.\n",
    "\n",
    "The communication infrastructure is called a Runtime.\n",
    "\n",
    "There are 2 types: **Standalone** and **Distributed**.\n",
    "\n",
    "Today we will use a standalone runtime: the **SingleThreadedAgentRuntime**, a local embedded agent runtime implementation.\n",
    "\n",
    "Tomorrow we'll briefly look at a Distributed runtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import dataclass for easily creating classes that store data\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Import core types and utilities from AutoGen Core:\n",
    "# AgentId: Used to uniquely identify an agent in the framework\n",
    "# MessageContext: Holds context information about the current message being processed\n",
    "# RoutedAgent: Base class for defining custom agents, with support for routing and handling messages\n",
    "# message_handler: Decorator to mark agent methods to automatically handle specific message types\n",
    "from autogen_core import AgentId, MessageContext, RoutedAgent, message_handler\n",
    "\n",
    "# Import the runtime implementation that executes agents and delivers messages in a local, single-threaded environment\n",
    "from autogen_core import SingleThreadedAgentRuntime\n",
    "\n",
    "# Import AssistantAgent, a prebuilt agent from the AgentChat module supporting conversational assistant logic\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "\n",
    "# Import TextMessage, the standard message type used to represent text content between agents in AgentChat\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "\n",
    "# Import OpenAIChatCompletionClient, a client class for interacting with OpenAI's chat completion APIs for generating responses\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we define our Message object\n",
    "\n",
    "Whatever structure we want for messages in our Agent framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use a minimal and clear message structure for our agent framework.\n",
    "# The @dataclass decorator allows us to easily define a lightweight class that simply holds data,\n",
    "# making our Message object immutable (unless otherwise specified) and auto-generating useful methods \n",
    "# like __init__, __repr__, and __eq__ behind the scenes.\n",
    "# Here, Message will just encapsulate the content of an exchange between agents.\n",
    "@dataclass\n",
    "class Message:\n",
    "    content: str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we define our Agent\n",
    "\n",
    "A subclass of RoutedAgent.\n",
    "\n",
    "Every Agent has an **Agent ID** which has 2 components:  \n",
    "`agent.id.type` describes the kind of agent it is  \n",
    "`agent.id.key` gives it its unique identifier\n",
    "\n",
    "Any method with the `@message_handler` decorated will have the opportunity to receive messages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define our class SimpleAgent that is a sub-class of RoutedAgent\n",
    "class SimpleAgent(RoutedAgent):\n",
    "    # In Python, the constructor method is always called __init__.\n",
    "    # It is called automatically when a new instance of the class is created.\n",
    "    # The first parameter of every instance method (including __init__) must be 'self', which refers to the instance itself.\n",
    "    # Constructors can take additional arguments after 'self' to initialize object state.\n",
    "    # Inside __init__, we usually initialize attributes or call the parent constructor with super().__init__()\n",
    "    def __init__(self) -> None:\n",
    "        # Here we call the parent class's constructor with a specific argument (\"Simple\").\n",
    "        # This argument typically sets the agent's type, which is important for agent identification\n",
    "        # and routing within the framework. By providing \"Simple\", we ensure this agent is recognized\n",
    "        # as a SimpleAgent throughout the system, allowing for correct handling, message dispatching,\n",
    "        # and potential differentiation from other agent types. Passing this value to the parent \n",
    "        # constructor lets the superclass logic manage shared setup behaviors while using this subclass's identity.\n",
    "        super().__init__(\"Simple\")\n",
    "\n",
    "    # Here we use the @message_handler decorator (imported from AutoGen Core), which enables this method to automatically receive messages\n",
    "    # intended for this agent, according to the agent framework's routing rules.\n",
    "    @message_handler\n",
    "    # TO BE NOTED: This method does not invoke an LLM or any external service; it simply processes the incoming Message\n",
    "    # and returns a new instance of our class Message with a simple string response constructed using the agent's id and input content.\n",
    "    async def on_my_message(self, message: Message, ctx: MessageContext) -> Message:\n",
    "        return Message(content=f\"This is {self.id.type}-{self.id.key}. You said '{message.content}' and I disagree.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OK let's create a Standalone runtime and register our agent type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentType(type='simple_agent')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we create a standalone runtime and we register our SimpleAgent class (the one withourt LLM)\n",
    "runtime = SingleThreadedAgentRuntime()\n",
    "# The lambda parameter is used here to provide a factory function that creates a new instance of SimpleAgent each time the agent is needed by the runtime.\n",
    "# This ensures that each agent has its own fresh state and avoids using a pre-created, potentially shared, instance.\n",
    "await SimpleAgent.register(\n",
    "    runtime, \"simple_agent\", \n",
    "    lambda: SimpleAgent()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alright! Let's start a runtime and send a message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we start the runtime\n",
    "runtime.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> This is simple_agent-default. You said 'Well hi there!' and I disagree.\n"
     ]
    }
   ],
   "source": [
    "# Create an AgentId object with type \"simple_agent\" and key \"default\" (REMEMBER: AgentId needs 2 components: type and key)\n",
    "# This identifies the agent we want to send a message to within the runtime system\n",
    "agent_id = AgentId(\n",
    "    type=\"simple_agent\", \n",
    "    key=\"default\"\n",
    ")\n",
    "\n",
    "# Send a message \"Well hi there!\" to the agent identified above, using the runtime,\n",
    "# and wait for the agent's response asynchronously. \n",
    "# Message() creates a message object to encapsulate the content for dispatch.\n",
    "response = await runtime.send_message(\n",
    "    message=Message(\"Well hi there!\"), \n",
    "    recipient=agent_id\n",
    ")\n",
    "\n",
    "# Output the response content received from the agent to the console, \n",
    "# prefixing it with \">>> \" for clarity in output display.\n",
    "# It will print \"You said 'Well hi there!' and I disagree\"\n",
    "print(\">>>\", response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we stop and close the single-threated runtime\n",
    "await runtime.stop()\n",
    "await runtime.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OK Now let's do something more interesting\n",
    "\n",
    "We'll use an AgentChat Assistant!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we build a more complex agent with LLM call\n",
    "class MyLLMAgent(RoutedAgent):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(\"LLMAgent\")\n",
    "        # IMPORTANT: \n",
    "        # In this agent class, we want to give our agent the power to interact with a large language model (LLM) such as OpenAI's GPT-4o-mini.\n",
    "        # To achieve this, we set up a \"model client\" that knows how to send and receive messages from the LLM.\n",
    "        # We then create an AssistantAgent (from the agentchat system) and store it privately on this agent as `self._delegate`.\n",
    "        # This means: whenever we want our MyLLMAgent to answer, it will internally use the delegate to call the LLM, \n",
    "        # letting our agent inherit powerful language capabilities without duplicating LLM-handling code.\n",
    "        # In summary: self._delegate wraps an LLM-powered AssistantAgent that MyLLMAgent can use to answer complex queries.\n",
    "        model_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n",
    "        self._delegate = AssistantAgent(name=\"LLMAgent\", model_client=model_client) # This method will be use to call the LLM thanks to AssistanAgent() class\n",
    "\n",
    "    # message_handler is a wrapper function to provide my funtion handle_my_message_type the capability to receive messages\n",
    "    @message_handler\n",
    "    # Parameters in Python functions are annotated with `:` to specify their types (type hints), not with `=` which is used for default values.\n",
    "    # The colon `:` after each parameter indicates its expected type, improving code readability and enabling type checking tools.\n",
    "    async def handle_my_message_type(self, message: Message, ctx: MessageContext) -> Message:\n",
    "        # Print out a message indicating that this agent type received an incoming message with its content shown\n",
    "        print(f\"{self.id.type} received message: {message.content}\")\n",
    "        # Convert the incoming message (from generic Message) into a TextMessage, indicating it's from the \"user\"\n",
    "        # TexMessage is an AutoGen Chat class for handle text messages types\n",
    "        text_message = TextMessage(content=message.content, source=\"user\")\n",
    "        # Asynchronously pass the TextMessage to the underlying LLM-powered delegate (AssistantAgent) to get a response\n",
    "        # Here we use the ._delegate method of our class MyLLMAgent to pass the message to the LLM\n",
    "        response = await self._delegate.on_messages(messages=[text_message], cancellation_token=ctx.cancellation_token)\n",
    "        # Extract the content of the reply from the response's chat_message attribute\n",
    "        reply = response.chat_message.content\n",
    "        # Print out what the agent is about to respond with, for debugging and clarity\n",
    "        print(f\"{self.id.type} responded: {reply}\")\n",
    "        # Return a new instance of Message object for the runtime to handle\n",
    "        # This coul be the input for another agent in case of agent workflow\n",
    "        return Message(content=reply)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentType(type='LLMAgent')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autogen_core import SingleThreadedAgentRuntime  # Import the SingleThreadedAgentRuntime which manages agent execution in a single thread\n",
    "\n",
    "runtime = SingleThreadedAgentRuntime()  # Instantiate a new single-threaded runtime environment for managing agent interactions\n",
    "\n",
    "# Register the SimpleAgent with the runtime:\n",
    "# - Uses the name \"simple_agent\" to identify this agent within the runtime.\n",
    "# - The lambda returns a new instance of SimpleAgent each time the agent is requested or spawned.\n",
    "await SimpleAgent.register(runtime=runtime, type=\"simple_agent\", factory=lambda: SimpleAgent())\n",
    "\n",
    "# Register the MyLLMAgent with the runtime:\n",
    "# - Uses the name \"LLMAgent\" to identify this LLM-capable agent type.\n",
    "# - The lambda returns a new instance of MyLLMAgent, ensuring each request gets a fresh agent.\n",
    "await MyLLMAgent.register(runtime=runtime, type=\"LLMAgent\", factory=lambda: MyLLMAgent())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-8' coro=<RunContext._run() running at /Users/federico.tognetti/Desktop/AgenticAIEngineering/agents/.venv/lib/python3.12/site-packages/autogen_core/_single_threaded_agent_runtime.py:110> wait_for=<Future pending cb=[Task.task_wakeup()]>>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMAgent received message: Hi there!\n",
      "LLMAgent responded: Hello! How can I assist you today?\n",
      ">>> Hello! How can I assist you today?\n",
      ">>> This is simple_agent-default. You said 'Hello! How can I assist you today?' and I disagree.\n",
      "LLMAgent received message: This is simple_agent-default. You said 'Hello! How can I assist you today?' and I disagree.\n",
      "LLMAgent responded: I appreciate your feedback! How would you prefer I assist you?\n",
      "LLMAgent received message: Which is the capital of Italy?\n",
      "LLMAgent responded: The capital of Italy is Rome.\n",
      ">>> The capital of Italy is Rome.\n"
     ]
    }
   ],
   "source": [
    "runtime.start()  # Start processing messages in the background.\n",
    "response = await runtime.send_message(message=Message(\"Hi there!\"), recipient=AgentId(type=\"LLMAgent\", key=\"default\"))\n",
    "print(\">>>\", response.content) # the LLM agent returns: Hello! How can I assist you today?\n",
    "response =  await runtime.send_message(message=Message(response.content), recipient=AgentId(type=\"simple_agent\", key=\"default\"))\n",
    "print(\">>>\", response.content) # the simple agent returns: This is simple_agent-default. You said 'Hello! How can I assist you today?' and I disagree.\n",
    "response = await runtime.send_message(message=Message(response.content), recipient=AgentId(type=\"LLMAgent\", key=\"default\"))\n",
    "# TO BE NOTED: I added these additional lines to better undestand how AutoGen Core operates in managing messages workflow\n",
    "response = await runtime.send_message(message=Message(\"Which is the capital of Italy?\"), recipient=AgentId(\"LLMAgent\", \"default\"))\n",
    "print(\">>>\", response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we stop and close the single-threaded runtime\n",
    "await runtime.stop()\n",
    "await runtime.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OK now let's show this at work - let's have 3 agents interact!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_ext.models.ollama import OllamaChatCompletionClient\n",
    "\n",
    "# Palyer 1 delegates to gpt-4o-mini\n",
    "class Player1Agent(RoutedAgent):\n",
    "    def __init__(self, name: str) -> None:\n",
    "        super().__init__(name)\n",
    "        model_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\", temperature=1.0)\n",
    "        self._delegate = AssistantAgent(name, model_client=model_client)\n",
    "\n",
    "    @message_handler\n",
    "    async def handle_my_message_type(self, message: Message, ctx: MessageContext) -> Message:\n",
    "        text_message = TextMessage(content=message.content, source=\"user\")\n",
    "        response = await self._delegate.on_messages([text_message], ctx.cancellation_token)\n",
    "        return Message(content=response.chat_message.content)\n",
    "    \n",
    "# Player 2 delegates to llama3.2\n",
    "class Player2Agent(RoutedAgent):\n",
    "    def __init__(self, name: str) -> None:\n",
    "        super().__init__(name)\n",
    "        model_client = OllamaChatCompletionClient(model=\"llama3.2\", temperature=1.0)\n",
    "        self._delegate = AssistantAgent(name, model_client=model_client)\n",
    "\n",
    "    @message_handler\n",
    "    async def handle_my_message_type(self, message: Message, ctx: MessageContext) -> Message:\n",
    "        text_message = TextMessage(content=message.content, source=\"user\")\n",
    "        response = await self._delegate.on_messages([text_message], ctx.cancellation_token)\n",
    "        return Message(content=response.chat_message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "JUDGE = \"You are judging a game of rock, paper, scissors. The players have made these choices:\\n\"\n",
    "\n",
    "class RockPaperScissorsAgent(RoutedAgent):\n",
    "    def __init__(self, name: str) -> None:\n",
    "        super().__init__(name)\n",
    "        model_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\", temperature=1.0)\n",
    "        self._delegate = AssistantAgent(name=name, model_client=model_client)\n",
    "\n",
    "    @message_handler\n",
    "    async def handle_my_message_type(self, message: Message, ctx: MessageContext) -> Message:\n",
    "        instruction = \"You are playing rock, paper, scissors. Respond only with the one word, one of the following: rock, paper, or scissors.\"\n",
    "        message = Message(content=instruction)\n",
    "        inner_1 = AgentId(type=\"player1\", key=\"default\")\n",
    "        inner_2 = AgentId(type=\"player2\", key=\"default\")\n",
    "        response1 = await self.send_message(message=message, recipient=inner_1)\n",
    "        response2 = await self.send_message(message=message, recipient=inner_2)\n",
    "        result = f\"Player 1: {response1.content}\\nPlayer 2: {response2.content}\\n\"\n",
    "        judgement = f\"{JUDGE}{result}Who wins?\"\n",
    "        # Tha following message is an instance of AutoGen chat\n",
    "        message = TextMessage(content=judgement, source=\"user\")\n",
    "        response = await self._delegate.on_messages([message], ctx.cancellation_token)\n",
    "        return Message(content=result + response.chat_message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime = SingleThreadedAgentRuntime()\n",
    "\n",
    "await Player1Agent.register(\n",
    "    runtime=runtime,\n",
    "    type=\"player1\",\n",
    "    factory=lambda: Player1Agent(\"player1\")\n",
    ")\n",
    "\n",
    "await Player2Agent.register(\n",
    "    runtime=runtime,\n",
    "    type=\"player2\",\n",
    "    factory=lambda: Player2Agent(\"player2\")\n",
    ")\n",
    "\n",
    "await RockPaperScissorsAgent.register(\n",
    "    runtime=runtime,\n",
    "    type=\"rock_paper_scissors\",\n",
    "    factory=lambda: RockPaperScissorsAgent(\"rock_paper_scissors\")\n",
    ")\n",
    "runtime.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 1: rock\n",
      "Player 2: scissors\n",
      "Player 1 wins because rock beats scissors. TERMINATE\n"
     ]
    }
   ],
   "source": [
    "agent_id = AgentId(\n",
    "    type=\"rock_paper_scissors\", \n",
    "    key=\"default\"\n",
    ")\n",
    "message = Message(content=\"go\")\n",
    "response = await runtime.send_message(message=message, recipient=agent_id)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "await runtime.stop()\n",
    "await runtime.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
